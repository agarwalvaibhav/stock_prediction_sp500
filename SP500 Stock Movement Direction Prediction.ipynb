{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import time\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "transform raw data into daily return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_null_rows(df, colname_list):\n",
    "    for colname in colname_list:\n",
    "        try:\n",
    "            df = df[df[colname]!=\"null\"].copy()\n",
    "        except:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def drop_zero_rows(df, colname_list):\n",
    "    for colname in colname_list:\n",
    "        df = df[df[colname]!=0].copy()\n",
    "    return df\n",
    "\n",
    "def get_close_close_data(path, min_sample, min_start_date):\n",
    "    df_list = []\n",
    "    for fname in os.listdir(path):\n",
    "        df = pd.read_csv(os.path.join(path, fname), sep = \",\")     \n",
    "        nb_rows = df.shape[0]\n",
    "    \n",
    "        # drop rows where \"null\" is appeared\n",
    "        df = drop_null_rows(df, [\"Close\", 'Open'])\n",
    "        dropped_null_rows = df.shape[0] - nb_rows\n",
    "        if dropped_null_rows > 0:\n",
    "            print(\"Dropped null rows at {}: {:d}\".format(fname, dropped_null_rows))\n",
    "\n",
    "        # only keep files where:\n",
    "        # - sample size > min_sample\n",
    "        # - first active date < min_start_date\n",
    "        if df.shape[0] > min_sample and df['Date'][0] <= min_start_date:\n",
    "            colname = fname.split(\".\")[0]\n",
    "            df[colname] = np.log(df['Close'].astype(float)/df['Close'].shift().astype(float))\n",
    "            df = df[['Date', colname]].copy()\n",
    "            df.set_index(\"Date\", inplace = True)\n",
    "            df_list.append(df)\n",
    "    return df_list\n",
    "\n",
    "def main_load_close_close(from_path, to_path, to_name, min_sample = 2000, min_start_date = \"2009-01-01\"):\n",
    "    df_list = get_close_close_data(path = from_path, min_sample = min_sample, min_start_date = min_start_date)\n",
    "    df_all = pd.concat(df_list, axis = 1)\n",
    "    \n",
    "    df_all = df_all.fillna(method='ffill')\n",
    "    df_all = df_all.loc[df_all.index >= min_start_date]\n",
    "    \n",
    "    df_all.to_csv(os.path.join(to_path, to_name), sep = \";\", header = True, index = True)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiaoy\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\ops.py:714: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = getattr(x, name)(y)\n"
     ]
    }
   ],
   "source": [
    "stock_return_df = main_load_close_close(from_path = \"shared_data/stocks\", to_path = \"shared_data\", to_name = \"sp_close_close_return.csv\")\n",
    "index_return_df = main_load_close_close(from_path = \"shared_data/index\", to_path = \"shared_data\", to_name = \"index_close_close_return.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of stocks kept: 463\n",
      "Indexes kept: ['aord', 'dax', 'djia', 'ftse', 'hangseng', 'nikkei', 'nyse', 'snp']\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb of stocks kept: {}\".format(stock_return_df.shape[1]))\n",
    "print(\"Indexes kept: {}\".format(list(index_return_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_one_lag_feature(mydf, colname, lag):\n",
    "    arr = mydf[colname]\n",
    "    feat_name = \"{}_lag_{:d}\".format(colname, lag)\n",
    "    lagged_arr = arr.shift(lag)\n",
    "    return feat_name, lagged_arr\n",
    "\n",
    "def build_lag_features(mydf, stock_name, lag_list = [1, 2, 3, 4, 5], verbose = False):\n",
    "    \"\"\"\n",
    "    Build lag features from given dataframe\n",
    "    \"\"\"\n",
    "    res_df = pd.DataFrame()\n",
    "    for lag in lag_list:\n",
    "        feat_name, lagged_arr = make_one_lag_feature(mydf, stock_name, lag)\n",
    "        res_df[feat_name] = lagged_arr\n",
    "    if verbose:\n",
    "        print(\"Feature set size:\", res_df.shape)\n",
    "    return res_df\n",
    "\n",
    "def main_save_lag_features(mydf, folder_name = \"self_lag_5\", lag_list = [1, 2, 3, 4, 5], verbose = False):\n",
    "    to_dir = os.path.join(\"shared_data\", \"features\", folder_name)\n",
    "    if not os.path.isdir(to_dir):\n",
    "        os.makedirs(to_dir)\n",
    "    for stock_name in mydf.columns:\n",
    "        feat_df = build_lag_features(mydf = mydf, stock_name = stock_name, lag_list = lag_list, verbose = verbose)\n",
    "        feat_df.to_csv(os.path.join(to_dir, \"{}.csv\".format(stock_name)), index = True, header = True, sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_save_lag_features(mydf = index_return_df, folder_name = \"index_lag_5\", lag_list = [1, 2, 3, 4, 5], verbose = False)\n",
    "main_save_lag_features(mydf = stock_return_df, folder_name = \"self_lag_5\", lag_list = [1, 2, 3, 4, 5], verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEAT_DIR = os.path.join(\"shared_data\", \"features\")\n",
    "RES_DIR = os.path.join(\"shared_data\", \"result\")\n",
    "TEST_RES_DIR = os.path.join(RES_DIR, \"testset\")\n",
    "# load the return data of all assets\n",
    "TARGET_DF = pd.read_csv(\"shared_data/sp_close_close_return.csv\", sep = \";\", header = 0, index_col = 0)\n",
    "TARGET_DF_INDEX = pd.read_csv(\"shared_data/index_close_close_return.csv\", sep = \";\", header = 0, index_col = 0)\n",
    "EUR_INDEX = [\"dax\", \"ftse\"]\n",
    "USA_INDEX = [\"djia\", \"nyse\", \"snp\"]\n",
    "ASIA_INDEX = [\"aord\", \"hangseng\", \"nikkei\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_feat_df(feat_df, verbose = True):\n",
    "    \"\"\"\n",
    "    Clena up raw feature df that may contain NA\n",
    "    \"\"\"\n",
    "    tot = feat_df.shape[0]\n",
    "    # drop rows containing na\n",
    "    feat_df.dropna(how = \"any\", inplace = True, axis = 0)\n",
    "    if verbose:\n",
    "        print(\"dropped na rows: \", tot - feat_df.shape[0])\n",
    "    \n",
    "    target = (feat_df['target'].values > 0).astype(int)\n",
    "    feat_df = feat_df.drop(\"target\", axis = 1)\n",
    "    return feat_df, target \n",
    "\n",
    "def load_features(asset_name, feature_name_list, feature_dir = FEAT_DIR, target_df = TARGET_DF, verbose = True, \n",
    "                  europe_lag = -1, asia_lag = -1, usa_lag = 0):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    asset_name: str\n",
    "        the name of the target asset\n",
    "    \n",
    "    feature_name_list: str\n",
    "        what feature sets we'll be using, each feature name corresponds to a folder name\n",
    "        if feature_name begins with \"self\", then we need to load the feature file with the name of the given asset,\n",
    "        if feature_name begins with \"index\", then load all features in given folde\n",
    "        \n",
    "    feature_dir: str\n",
    "        location of feature data\n",
    "    \n",
    "    verbose: bool\n",
    "        whether to show execution messages\n",
    "        \n",
    "    europe_lag: int\n",
    "        additional lag applied on europe data\n",
    "    \n",
    "    asia_lag: int\n",
    "        additional lag applied on asia data\n",
    "    \n",
    "    usa_lag: int\n",
    "        additional lag applied on usa data\n",
    "    \"\"\"\n",
    "    feat_df_list = []\n",
    "    if verbose:\n",
    "        print(\"******* loading features on {} *******\".format(asset_name))\n",
    "    for feature_name in feature_name_list:\n",
    "        if re.search(\"^self_\", feature_name):\n",
    "            # if the feature name begins with self_, then just load corresponding data file\n",
    "            feat_path = os.path.join(feature_dir, feature_name, \"{}.csv\".format(asset_name))\n",
    "            df = pd.read_csv(feat_path, sep = \";\", header = 0, index_col = 0)\n",
    "            feat_df_list.append(df)\n",
    "        elif re.search(\"^index_\", feature_name):\n",
    "            # if the feature_name begins with index_, use all the features in the folder as features\n",
    "            current_dir = os.path.join(feature_dir,  feature_name)\n",
    "            for fname in os.listdir(current_dir):\n",
    "                feat_path = os.path.join(current_dir, fname)\n",
    "                df = pd.read_csv(feat_path, sep = \";\", header = 0, index_col = 0)\n",
    "                if re.search(\"|\".join(EUR_INDEX), fname) and (europe_lag != 0):\n",
    "                    # europe market data\n",
    "                    df = df.shift(europe_lag)\n",
    "                    df.columns = [\"{}_lag_{}\".format(x, europe_lag) for x in df.columns]\n",
    "                elif re.search(\"|\".join(ASIA_INDEX), fname) and (asia_lag != 0):\n",
    "                    df = df.shift(asia_lag)\n",
    "                    df.columns = [\"{}_lag_{}\".format(x, asia_lag) for x in df.columns]\n",
    "                elif re.search(\"|\".join(USA_INDEX), fname) and (usa_lag != 0):\n",
    "                    df = df.shift(usa_lag)\n",
    "                    df.columns = [\"{}_lag_{}\".format(x, usa_lag) for x in df.columns]\n",
    "                \n",
    "                feat_df_list.append(df)\n",
    "        \n",
    "    feat_df = pd.concat(feat_df_list, axis = 1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"raw feature shape: {}\".format(feat_df.shape))\n",
    "    \n",
    "    feat_df['target'] = target_df[asset_name]\n",
    "    \n",
    "    feat_df, target = format_feat_df(feat_df, verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"cleaned feature shape: {}\".format(feat_df.shape))\n",
    "    \n",
    "    return feat_df, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_split_data(X, y, mode, nb_days = 252):\n",
    "    \"\"\"\n",
    "    split data for validation purpose (or asset selection)\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    mode: str\n",
    "        - \"cv\": classic 5 folds cross validation\n",
    "        - \"test\", classic train test split, using one year of data as test \n",
    "        - \"ts\", time series based 5 folds sequential splitting\n",
    "    nb_days: int\n",
    "        nb of trading days a year\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    out: list\n",
    "        list of 4-tuples (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    train_test_list = []\n",
    "    \n",
    "    if mode == \"single\":\n",
    "        X_train = X[:nb_days, :]\n",
    "        X_test = X[-nb_days:, :]\n",
    "\n",
    "        y_train = y[:nb_days]\n",
    "        y_test = y[-nb_days:]\n",
    "        \n",
    "        train_test_list.append((X_train, y_train, X_test, y_test))\n",
    "    elif mode == \"cv\":\n",
    "        cv = KFold(n_splits = 5, shuffle = False, random_state = 2017)\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            train_test_list.append((X_train, y_train, X_test, y_test))\n",
    "    elif mode == \"ts\":\n",
    "        ts = TimeSeriesSplit(n_splits = 5)\n",
    "        for train_index, test_index in ts.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            train_test_list.append((X_train, y_train, X_test, y_test))\n",
    "    else:\n",
    "        print(\"split mode undefined: {}\".format(mode))\n",
    "        sys.exit(1)\n",
    "    return train_test_list \n",
    "\n",
    "def main_split_data(X, y, mode, nb_days = 252):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    X: numpy ndarray\n",
    "        training variables\n",
    "    y: array like\n",
    "        target variable\n",
    "    mode: str\n",
    "        \"cv\", \"test\" or \"ts\"\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    out: (4-tuple, list of 4-tuples)\n",
    "        (X_train, y_train, X_test, y_test), list of 4 tuples for validation purpose\n",
    "    \"\"\"\n",
    "    # First split out test set which is the last one year data\n",
    "    X_train = X[:-nb_days, :]\n",
    "    X_test = X[-nb_days:, :]\n",
    "    \n",
    "    # this test set will be used to compare all strategies\n",
    "    y_train = y[:-nb_days]\n",
    "    y_test = y[-nb_days:]\n",
    "    \n",
    "    # split the training set for validation purpose\n",
    "    validation_list = validation_split_data(X = X_train, y = y_train, mode = mode, nb_days = nb_days)\n",
    "    return (X_train, y_train, X_test, y_test), validation_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_on_validation_list(validation_list, clf, out_name, folder_name, out_dir = RES_DIR, verbose = True, to_save = True):\n",
    "    \"\"\"\n",
    "    Run prediction and save result to disk \n",
    "    \n",
    "    Params:\n",
    "    --------\n",
    "    validation_list: list of 4-tuples\n",
    "        (X_train, y_train, X_test, y_test) tuples\n",
    "    clf: predictor\n",
    "        fit and predict\n",
    "    out_name: str\n",
    "        result file name, usaully set to asset name itself\n",
    "    folder_name: str\n",
    "        the method name used as the folder name to save all the prediction results \n",
    "    \n",
    "    \"\"\"\n",
    "    # res_df = pd.DataFrame()\n",
    "    to_dir = os.path.join(out_dir, folder_name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"**** Running Prediction on {} with {} *****\".format(out_name, folder_name))\n",
    "    \n",
    "    if not os.path.isdir(to_dir):\n",
    "        os.makedirs(to_dir)\n",
    "    \n",
    "    out_path = os.path.join(to_dir, \"{}.csv\".format(out_name))\n",
    "    acc_list, auc_list = [], []\n",
    "    \n",
    "    res_df_list = []\n",
    "    for i, (X_train, y_train, X_test, y_test) in enumerate(validation_list):\n",
    "        res_df = pd.DataFrame()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        res_df[\"true_{}\".format(i)] = y_test\n",
    "        res_df[\"pred_{}\".format(i)] = y_pred\n",
    "        \n",
    "        res_df_list.append(res_df)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "        \n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "            \n",
    "    res_df = pd.concat(res_df_list, axis = 1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Result on {} - avg AUC: {:0.4f}, avg Accuracy: {:0.4f}\".format(out_name, np.mean(auc_list), np.mean(acc_list)))\n",
    "    \n",
    "    if to_save:\n",
    "        res_df.to_csv(out_path, index = False, header = True, sep = \";\")\n",
    "    \n",
    "    return np.mean(auc_list)\n",
    "\n",
    "def main_predict_all_stocks_on_validation(feature_name_list, \n",
    "                                   validation_mode, \n",
    "                                   clf, \n",
    "                                   clf_name,                                  \n",
    "                                   verbose = True, \n",
    "                                   nn_param_dict = None):\n",
    "    \"\"\"\n",
    "    Run prediction on all assets\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    feature_name_list: list of str\n",
    "        list of feature set to use\n",
    "    \n",
    "    validation_mode: str\n",
    "        \"test\", \"cv\", or \"ts\". It defines which validation scheme to use\n",
    "    \n",
    "    clf: predictor\n",
    "    \n",
    "    clf_name: str\n",
    "        predictor's name\n",
    "    \n",
    "    verbose: bool\n",
    "        whether to show execution messages\n",
    "        \n",
    "    \"\"\"\n",
    "    # method_name identifies the current approach \n",
    "   \n",
    "    method_name = \"-\".join([\"+\".join(feature_name_list), clf_name, validation_mode])\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # df_raw will be used to extract target variables\n",
    "   \n",
    "    df_raw = TARGET_DF\n",
    "   \n",
    "    # get the name of all assets\n",
    "    asset_list = df_raw.columns\n",
    "    tot = len(asset_list)\n",
    "    for i, asset_name in enumerate(asset_list):\n",
    "        # load feature set\n",
    "        to_path = os.path.join(RES_DIR, method_name, \"{}.csv\".format(asset_name))\n",
    "       \n",
    "        feat_df, target = load_features(\n",
    "                                        asset_name = asset_name, \n",
    "                                        feature_name_list = feature_name_list, \n",
    "                                        feature_dir = FEAT_DIR, \n",
    "                                        target_df = df_raw,\n",
    "                                        verbose = verbose)\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(feat_df.values)\n",
    "        y = np.array(list(target))\n",
    "\n",
    "        # split\n",
    "        (X_train, y_train, X_test, y_test), validation_list = main_split_data(X = X, y = y, mode = validation_mode)\n",
    "\n",
    "        # run prediction\n",
    "        predict_on_validation_list(validation_list = validation_list, \n",
    "                                   clf = clf, \n",
    "                                   out_name = asset_name, \n",
    "                                   folder_name = method_name,\n",
    "                                   out_dir = RES_DIR, \n",
    "                                   verbose = False, \n",
    "                                   to_save = True)\n",
    "        if verbose:\n",
    "            print(\"{}/{} Ended\".format(i + 1, tot))\n",
    "    \n",
    "    print(\"Total time spend: {} mins\".format((time.time() - t0)//60))\n",
    "\n",
    "def main_predict_sp500_index_on_validation(feature_name_list, validation_mode, \n",
    "                                     clf, clf_name, to_std = True, \n",
    "                                     verbose = True, \n",
    "                                     nn_param_dict = None, \n",
    "                                     to_save = True):\n",
    "    \"\"\"\n",
    "    We only predict the sp500 index for now\n",
    "    \"\"\"\n",
    "    \n",
    "    asset_name = \"snp\"\n",
    "   \n",
    "    method_name = \"-\".join([\"+\".join(feature_name_list), clf_name, validation_mode])\n",
    "    \n",
    "    df_raw = TARGET_DF_INDEX\n",
    "    \n",
    "    to_path = os.path.join(RES_DIR, method_name, \"{}.csv\".format(asset_name))\n",
    "    \n",
    "   \n",
    "    feat_df, target = load_features(\n",
    "                                    asset_name = asset_name, \n",
    "                                    feature_name_list = feature_name_list, \n",
    "                                    feature_dir = FEAT_DIR, \n",
    "                                    target_df = df_raw,\n",
    "                                    verbose = False)\n",
    "\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(feat_df.values)\n",
    "    y = np.array(list(target))\n",
    "\n",
    "    # split\n",
    "    (X_train, y_train, X_test, y_test), validation_list = main_split_data(X = X, y = y, mode = validation_mode)\n",
    "\n",
    "    # run prediction\n",
    "    return predict_on_validation_list(validation_list = validation_list, \n",
    "                               clf = clf, \n",
    "                               out_name = asset_name, \n",
    "                               folder_name = method_name,\n",
    "                               out_dir = RES_DIR, \n",
    "                               verbose = False, \n",
    "                               to_save = to_save)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spend: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "lr_best = LogisticRegression(penalty = \"l1\", C = 0.05)\n",
    "main_predict_all_stocks_on_validation(feature_name_list = [\"self_lag_5\", \"index_lag_5\"], \n",
    "                               validation_mode = \"ts\", \n",
    "                               clf = lr_best, \n",
    "                               clf_name = \"logistic-regression\", \n",
    "                               verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_list_from_dir(folder_name, result_dir = RES_DIR):\n",
    "    from_dir = os.path.join(result_dir, folder_name)\n",
    "    auc_list = []\n",
    "    for filename in os.listdir(from_dir):\n",
    "        df = pd.read_csv(os.path.join(from_dir, filename), sep = \";\", header = 0, index_col = None)\n",
    "        nb = int(df.shape[1]/2)\n",
    "        tmp_auc_list = []\n",
    "        for i in range(nb):\n",
    "            y_pred = df['pred_{}'.format(i)].values\n",
    "            y_test = df['true_{}'.format(i)].values\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            tmp_auc_list.append(auc)\n",
    "        auc = np.mean(tmp_auc_list)\n",
    "        auc_list.append(auc)\n",
    "    return auc_list  \n",
    "\n",
    "def show_overall_performance(folder_name, result_dir = RES_DIR, show_hist = False):\n",
    "    from_dir = os.path.join(result_dir, folder_name)\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    for filename in os.listdir(from_dir):\n",
    "        df = pd.read_csv(os.path.join(from_dir, filename), sep = \";\", header = 0, index_col = None)\n",
    "        nb = int(df.shape[1]/2)\n",
    "        tmp_auc_list = []\n",
    "        tmp_acc_list = []\n",
    "        for i in range(nb):\n",
    "            y_pred = df['pred_{}'.format(i)].values\n",
    "            y_test = df['true_{}'.format(i)].values\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            tmp_auc_list.append(auc)\n",
    "            acc = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
    "            tmp_acc_list.append(acc)\n",
    "        \n",
    "        auc_list.append(np.mean(tmp_auc_list))\n",
    "        acc_list.append(np.mean(tmp_acc_list))\n",
    "    \n",
    "    print(\"Result on {} - avg AUC: {:0.4f} with std: {:0.4f}, avg Accuracy: {:0.4f} with std: {:0.4f}\".\\\n",
    "          format(folder_name, np.mean(auc_list), np.std(auc_list), np.mean(acc_list), np.std(acc_list))) \n",
    "    if show_hist:\n",
    "        plt.hist(auc_list, bins = 30, alpha=0.5, label=\"AUC\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "    \n",
    "    return np.mean(auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on self_lag_5+index_lag_5-logistic-regression-ts - avg AUC: 0.6942 with std: 0.0381, avg Accuracy: 0.6376 with std: 0.0294\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQUz5CWpYIa8JBXMkJekgv1/7HTtfsyuMxgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d4b6a4cf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.69424496743601383"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_overall_performance(folder_name = \"self_lag_5+index_lag_5-logistic-regression-ts\", show_hist = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
